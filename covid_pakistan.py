# -*- coding: utf-8 -*-
"""Covid_Pakistan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UMbnmhlihbIQLmFBY-7sv-XOJqnZT_2N
"""

pip install statsmodels --upgrade

import pandas as pd
import numpy as np
from matplotlib import pyplot
from statsmodels.tsa.ar_model import AutoReg

from google.colab import files
uploaded = files.upload()

data=pd.read_csv('Covid_Pakistan.csv')
data

cases=pd.DataFrame(data,columns=['total_cases'])
cases

X=cases.values
Y=data['date'].values

from statsmodels.tsa.stattools import adfuller
df_test=adfuller(cases['total_cases'],autolag='AIC')
print("1. ADF :", df_test[0])
print("2. P-Value :",df_test[1])
print("3. No of Lags :",df_test[2])
print("4. No of Observation :",df_test[3])
print("5. Critical Values :",df_test[4])
for key,val in df_test[4].items():
  print("\t",key,": ",val)

import matplotlib.pyplot as plt 
from statsmodels.tsa.seasonal import seasonal_decompose

newData=pd.DataFrame(X,Y)

newData=newData.rename(columns={0:'Cases'})

logged_newData = newData['Cases'].apply(lambda x : np.log(x))

ax1 = plt.subplot(121)
logged_newData.plot(figsize=(12,4) ,color="tab:red", title="Log Transformed Values", ax=ax1);
ax2 = plt.subplot(122)
cases.plot(color="tab:red", title="Original Values", ax=ax2);

decompose_result =seasonal_decompose(newData,period=1)
decompose_result.plot();

powered_newData = newData["Cases"].apply(lambda x : x ** 0.5)

ax1 = plt.subplot(121)
powered_newData.plot(figsize=(12,4), color="tab:red", title="Powered Transformed Values", ax=ax1);
ax2 = plt.subplot(122)
newData.plot(figsize=(12,4), color="tab:red", title="Original Values", ax=ax2)

decompose_result = seasonal_decompose(powered_newData,period=1)

decompose_result.plot();

rolling_mean = newData.rolling(window = 12).mean()
Cases_rolled_detrended = newData - rolling_mean

ax1 = plt.subplot(121)
Cases_rolled_detrended.plot(figsize=(12,4),color="tab:red", title="Differenced With Rolling Mean over 12 month", ax=ax1);
ax2 = plt.subplot(122)
newData.plot(figsize=(12,4), color="tab:red", title="Original Values", ax=ax2);

decompose_result = seasonal_decompose(Cases_rolled_detrended.dropna(),period=1)

decompose_result.plot();

logged_newData = pd.DataFrame(newData["Cases"].apply(lambda x : np.log(x)))

rolling_mean = logged_newData.rolling(window = 12).mean()
cases_log_rolled_detrended = logged_newData["Cases"] - rolling_mean["Cases"]


ax1 = plt.subplot(121)
cases_log_rolled_detrended.plot(figsize=(12,4),color="tab:red", title="Log Transformation & Differenced With Rolling Mean over 12 month", ax=ax1);
ax2 = plt.subplot(122)
newData.plot(figsize=(12,4), color="tab:red", title="Original Values", ax=ax2);

decompose_result = seasonal_decompose(cases_log_rolled_detrended.dropna(),period=1)

decompose_result.plot();

powered_cases = pd.DataFrame(newData["Cases"].apply(lambda x : np.log(x)))

rolling_mean = powered_cases.rolling(window = 12).mean()
cases_pow_rolled_detrended = powered_cases["Cases"] - rolling_mean["Cases"]

ax1 = plt.subplot(121)
cases_pow_rolled_detrended.plot(figsize=(12,4),color="tab:red", title="Power Transformation & Differenced With Rolling Mean over 12 month", ax=ax1);
ax2 = plt.subplot(122)
newData.plot(figsize=(12,4), color="tab:red", title="Original Values", ax=ax2);

decompose_result = seasonal_decompose(cases_pow_rolled_detrended.dropna(),period=1)

decompose_result.plot();

ThisList=[]
for i in range(newData.shape[0]):
  ThisList.append(i)
ThisList

from statsmodels.regression.linear_model import OLS
least_squares = OLS(newData["Cases"].values , ThisList)
result = least_squares.fit()

fit = pd.Series(result.predict(ThisList), index = newData.index)

cases_ols_detrended = newData["Cases"] - fit

ax1 = plt.subplot(121)
cases_ols_detrended.plot(figsize=(12,4), color="tab:red", title="Linear Regression Fit", ax=ax1);
ax2 = plt.subplot(122)
newData.plot(figsize=(12,4), color="tab:red", title="Original Values", ax=ax2);

logged_newData_diff = logged_newData - logged_newData.shift()

ax1 = plt.subplot(121)
logged_newData_diff.plot(figsize=(12,4), color="tab:red", title="Log-Transformed & Differenced Time-Series", ax=ax1)
ax2 = plt.subplot(122)
newData.plot(figsize=(12,4), color="tab:red", title="Original Values", ax=ax2);

logged_newData_diff=logged_newData_diff.replace([np.inf, -np.inf], np.nan)
logged_newData_diff=logged_newData_diff.dropna()

dftest = adfuller(logged_newData_diff["Cases"].values, autolag = 'AIC')

print("1. ADF : ",dftest[0])
print("2. P-Value : ", dftest[1])
print("3. Num Of Lags : ", dftest[2])
print("4. Num Of Observations Used For ADF Regression and Critical Values Calculation :", dftest[3])
print("5. Critical Values :")
for key, val in dftest[4].items():
    print("\t",key, ": ", val)

powered_newData_diff = powered_newData - powered_newData.shift()

ax1 = plt.subplot(121)
powered_newData_diff.plot(figsize=(12,4), color="tab:red", title="Power-Transformed & Differenced Time-Series", ax=ax1);
ax2 = plt.subplot(122)
newData.plot(figsize=(12,4), color="tab:red", title="Original Values", ax=ax2);

dftest = adfuller(powered_newData_diff.dropna().values, autolag = 'AIC')
print("1. ADF : ",dftest[0])
print("2. P-Value : ", dftest[1])
print("3. Num Of Lags : ", dftest[2])
print("4. Num Of Observations Used For ADF Regression and Critical Values Calculation :", dftest[3])
print("5. Critical Values :")
for key, val in dftest[4].items():
    print("\t",key, ": ", val)

cases_rolled_detrended_diff = Cases_rolled_detrended - Cases_rolled_detrended.shift()

ax1 = plt.subplot(121)
cases_rolled_detrended_diff.plot(figsize=(8,4), color="tab:red", title="Rolled & Differenced Time-Series", ax=ax1);
ax2 = plt.subplot(122)
newData.plot(figsize=(12,4), color="tab:red", title="Original Values", ax=ax2);

dftest = adfuller(cases_rolled_detrended_diff.dropna().values, autolag = 'AIC')

print("1. ADF : ",dftest[0])
print("2. P-Value : ", dftest[1])
print("3. Num Of Lags : ", dftest[2])
print("4. Num Of Observations Used For ADF Regression and Critical Values Calculation :", dftest[3])
print("5. Critical Values :")
for key, val in dftest[4].items():
    print("\t",key, ": ", val)

print(cases_rolled_detrended_diff.dropna())

cases_rolled_detrended_diff=cases_rolled_detrended_diff.dropna()
newData=cases_rolled_detrended_diff

"""# 1.Auto Regression"""

from statsmodels.tsa.stattools import adfuller
df_test=adfuller(newData['Cases'],autolag='AIC')
print("1. ADF :", df_test[0])
print("2. P-Value :",df_test[1])
print("3. No of Lags :",df_test[2])
print("4. No of Observation :",df_test[3])
print("5. Critical Values :",df_test[4])
for key,val in df_test[4].items():
  print("\t",key,": ",val)

from statsmodels.graphics.tsaplots import plot_pacf,plot_acf
pacf=plot_pacf(newData,lags=25)
acf=plot_acf(newData,lags=25)

train=X[:len(X)-7]
test=X[len(X)-7:]

model=AutoReg(train,lags=10).fit()

print(model.summary())

pred=model.predict(start=len(train),end=len(X)-1,dynamic=False)
pyplot.plot(pred)
pyplot.plot(test,color='red')
print(pred)

from math import sqrt
RMSE_List=[]
from sklearn.metrics import mean_squared_error
rmse=sqrt(mean_squared_error(test,pred))
RMSE_List.append((rmse))

print(rmse)

pred_future=model.predict(start=len(X)+1,end=len(X)+7,dynamic=False)
print("Future Prediction for the next week")
print(pred_future)
print("No of prediction Made:",len(pred_future))

"""# **2.ARIMA MODEL**"""

pip install pmdarima

from pmdarima import auto_arima
stepwise_fit = auto_arima(newData['Cases'], trace=True,
suppress_warnings=True)

print(newData.shape)
train=newData.iloc[:-30]
test=newData.iloc[-30:]
print(train.shape,test.shape)

from statsmodels.tsa.arima_model import ARIMA
model=ARIMA(newData['Cases'],order=(1,0,5))
model=model.fit()
model.summary()

start=len(train)
end=len(train)+len(test)-1
pred=model.predict(start=start,end=end,typ='levels').rename('ARIMA Predictions')
plt.plot(pred)
plt.plot(test)
plt.legend(['Test Data', 'ARIMA Model Predictions'])

from sklearn.metrics import mean_squared_error
from math import sqrt
test.mean()
rmse=sqrt(mean_squared_error(pred,test))
print(rmse)
RMSE_List.append((rmse))

"""# *3.ARMA MODEL*"""

from statsmodels.tsa.arima_model import ARMA

model=ARMA(newData['Cases'],order=(4,1))
model=model.fit()
model.summary()

start=len(train)
end=len(train)+len(test)-1
pred=model.predict(start=start,end=end,typ='levels').rename('ARMA Predictions')
plt.plot(pred)
plt.plot(test)
plt.legend(['Test Data', 'ARMA Model Predictions'])

from sklearn.metrics import mean_squared_error
from math import sqrt
test.mean()
rmse=sqrt(mean_squared_error(pred,test))
print(rmse)
RMSE_List.append((rmse))

"""# 4.SariMaX Model"""

from statsmodels.tsa.statespace.sarimax import SARIMAX

myorder=(0,3,0)
seasonal_order=(0,3,0,12)
model=SARIMAX(newData['Cases'],order=myorder,seasonal_order=seasonal_order)

model=model.fit()
model.summary()

start=len(train)
end=len(train)+len(test)-1
pred=model.predict(start=start,end=end,typ='levels').rename('SARIMAX Predictions')
plt.plot(pred)
plt.plot(test)
plt.legend(['Test Data', 'SARIMAX Model Predictions'])

from sklearn.metrics import mean_squared_error
from math import sqrt
test.mean()
rmse=sqrt(mean_squared_error(pred,test))
print(rmse)
RMSE_List.append((rmse))

"""## 5.SVM Prediction"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import matplotlib.pyplot as plt 
import matplotlib.colors as mcolors
import pandas as pd 
import random
import math
import time
from sklearn.linear_model import LinearRegression, BayesianRidge
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error
import datetime
import operator 
plt.style.use('fivethirtyeight')
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")

dates=newData.index

days_since_1_22 = np.array([i for i in range(len(dates))]).reshape(-1, 1)
world_cases = np.array(newData).reshape(-1, 1)

X_train_confirmed, X_test_confirmed, y_train_confirmed, y_test_confirmed = train_test_split(days_since_1_22[50:],newData[50:], test_size=0.05, shuffle=False)

newData.index

days_in_future = 10
future_forcast = np.array([i for i in range(len(dates)+days_in_future)]).reshape(-1, 1)
adjusted_dates = future_forcast[:-10]

# svm_confirmed = svm_search.best_estimator_
svm_confirmed = SVR(shrinking=True, kernel='poly',gamma=0.01, epsilon=1,degree=3, C=0.1)
svm_confirmed.fit(X_train_confirmed, y_train_confirmed)
svm_pred = svm_confirmed.predict(future_forcast)

# check against testing data
svm_test_pred = svm_confirmed.predict(X_test_confirmed)
print('MAE:', mean_absolute_error(svm_test_pred, y_test_confirmed))
print('MSE:',mean_squared_error(svm_test_pred, y_test_confirmed))
rmse=sqrt(mean_squared_error(svm_test_pred, y_test_confirmed))
RMSE_List.append((rmse))
RMSE_List.append(rmse)
plt.plot(y_test_confirmed)
plt.plot(svm_test_pred)

plt.plot(svm_test_pred)

"""# `6.Polynomial regression`"""

# transform our data for polynomial regression
poly = PolynomialFeatures(degree=4)
poly_X_train_confirmed = poly.fit_transform(X_train_confirmed)
poly_X_test_confirmed = poly.fit_transform(X_test_confirmed)
poly_future_forcast = poly.fit_transform(future_forcast)

bayesian_poly = PolynomialFeatures(degree=5)
bayesian_poly_X_train_confirmed = bayesian_poly.fit_transform(X_train_confirmed)
bayesian_poly_X_test_confirmed = bayesian_poly.fit_transform(X_test_confirmed)
bayesian_poly_future_forcast = bayesian_poly.fit_transform(future_forcast)

# polynomial regression
linear_model = LinearRegression(normalize=True, fit_intercept=False)
linear_model.fit(poly_X_train_confirmed, y_train_confirmed)
test_linear_pred = linear_model.predict(poly_X_test_confirmed)
linear_pred = linear_model.predict(poly_future_forcast)
print('MAE:', mean_absolute_error(test_linear_pred, y_test_confirmed))
print('MSE:',mean_squared_error(test_linear_pred, y_test_confirmed))
rmse=sqrt(mean_squared_error(test_linear_pred, y_test_confirmed))
RMSE_List.append(rmse)

print(linear_model.coef_)

plt.plot(y_test_confirmed)
plt.plot(test_linear_pred)

"""# 7.Bayesian Ridge Polynomial Regression"""

# bayesian ridge polynomial regression
tol = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]
alpha_1 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]
alpha_2 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]
lambda_1 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]
lambda_2 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]
normalize = [True, False]
bayesian_grid = {'tol': tol, 'alpha_1': alpha_1, 'alpha_2' : alpha_2, 'lambda_1': lambda_1, 'lambda_2' : lambda_2, 
                 'normalize' : normalize}
bayesian = BayesianRidge(fit_intercept=False)
bayesian_search = RandomizedSearchCV(bayesian, bayesian_grid, scoring='neg_mean_squared_error', cv=3, return_train_score=True, n_jobs=-1, n_iter=40, verbose=1)
bayesian_search.fit(bayesian_poly_X_train_confirmed, y_train_confirmed)

bayesian_search.best_params_

bayesian_confirmed = bayesian_search.best_estimator_
test_bayesian_pred = bayesian_confirmed.predict(bayesian_poly_X_test_confirmed)
bayesian_pred = bayesian_confirmed.predict(bayesian_poly_future_forcast)
print('MAE:', mean_absolute_error(test_bayesian_pred, y_test_confirmed))
print('MSE:',mean_squared_error(test_bayesian_pred, y_test_confirmed))
rmse=sqrt(mean_squared_error(test_linear_pred, y_test_confirmed))
RMSE_List.append(rmse)

plt.plot(y_test_confirmed['Cases'])
plt.plot(test_bayesian_pred)

"""# Appling Other Regression"""





import sklearn.metrics as metrics
def regression_results(y_true, y_pred):
# Regression metrics
    explained_variance=metrics.explained_variance_score(y_true, y_pred)
    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) 
    mse=metrics.mean_squared_error(y_true, y_pred) 
    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)
    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)
    r2=metrics.r2_score(y_true, y_pred)
    print('explained_variance: ', round(explained_variance,4))    
    print('mean_squared_log_error: ', round(mean_squared_log_error,4))
    print('r2: ', round(r2,4))
    print('MAE: ', round(mean_absolute_error,4))
    print('MSE: ', round(mse,4))
    print('RMSE: ', round(np.sqrt(mse),4))

data_cases = newData[['Cases']]
data_cases.loc[:,'Yesterday'] = data_cases.loc[:,'Cases'].shift()
data_cases.loc[:,'Yesterday_Diff'] = data_cases.loc[:,'Yesterday'].diff()
data_cases = data_cases.dropna()

data_cases

X_train = data_cases[:'30/09/2020'].drop(['Cases'], axis = 1)
y_train = data_cases.loc[:'30/09/2020', 'Cases']
X_test = data_cases['01/09/2020':].drop(['Cases'], axis = 1)
y_test = data_cases.loc['01/09/2020':, 'Cases']

X_train

y_train

X_test

y_test

X_train = abs(X_train)
y_train = abs(y_train)
X_test = abs(X_test)
y_test=abs(y_test)

from sklearn.model_selection import TimeSeriesSplit
from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

models = []
models.append(('LR', LinearRegression()))
models.append(('NN', MLPRegressor(solver = 'lbfgs')))  #neural network
models.append(('KNN', KNeighborsRegressor())) 
models.append(('RF', RandomForestRegressor(n_estimators = 10))) # Ensemble method - collection of many decision trees
models.append(('SVR', SVR(gamma='auto'))) # kernel = linear
# Evaluate each model in turn
results = []
names = []
for name, model in models:
    # TimeSeries Cross validation
 tscv = TimeSeriesSplit(n_splits=10)
    
 cv_results = cross_val_score(model, X_train, y_train, cv=tscv, scoring='r2')
 results.append(cv_results)
 names.append(name)
 print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))
    
# Compare Algorithms
plt.boxplot(results, labels=names)
plt.title('Algorithm Comparison')
plt.show()

from sklearn.metrics import mean_squared_error

from sklearn.metrics import make_scorer
def rmse(actual, predict):
  predict = np.array(predict)
  actual = np.array(actual)
  distance = predict - actual
  square_distance = distance ** 2
  mean_square_distance = square_distance.mean()
  score = np.sqrt(mean_square_distance)
  return score
rmse_score = make_scorer(rmse, greater_is_better = False)

from sklearn.model_selection import GridSearchCV
model = RandomForestRegressor()
param_search = { 
    'n_estimators': [20, 50, 100],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [i for i in range(5,15)]
}
tscv = TimeSeriesSplit(n_splits=10)
gsearch = GridSearchCV(estimator=model, cv=tscv, param_grid=param_search, scoring = rmse_score)
gsearch.fit(X_train, y_train)
best_score = gsearch.best_score_
best_model = gsearch.best_estimator_

y_true = y_test.values
y_true=abs(y_true)
y_pred = best_model.predict(X_test)
y_pred=abs(y_pred)
regression_results(y_true, y_pred)

from google.colab import drive
drive.mount('drive')

import pickle

pickle.dump(best_model,open('model.pkl','wb'))

!cp model.pkl "drive/My Drive/"

model=pickle.load(open('model.pkl','rb'))

newCheck={'Yesterday':3200,'Yesterday_diff':65}
newCheck=pd.DataFrame(newCheck,index=[0])
newCheck

best_model.predict(newCheck)

